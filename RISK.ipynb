{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"RISK.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"BkSQAqNahFSG","colab":{"base_uri":"https://localhost:8080/","height":751},"executionInfo":{"status":"ok","timestamp":1583905394447,"user_tz":-330,"elapsed":16866,"user":{"displayName":"Chat Goo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjb90mL2nb5uUPlC7acaRzczH5y0WWbcyUz-dkuSw=s64","userId":"10971415490752751732"}},"outputId":"70777fe1-cbff-4c50-b5e9-ac3bc3bfc5b9"},"source":["# Install packages not preinstalled in jupyter notebook (RUN ONCE)\n","\n","!pip install xgboost\n","!pip install seaborn\n","!pip install yellowbrick\n","!pip install category_encoders"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.17.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.0)\n","Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.25.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.17.5)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n","Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.1.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2.6.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (45.2.0)\n","Requirement already satisfied: yellowbrick in /usr/local/lib/python3.6/dist-packages (0.9.1)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.22.1)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.10.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (3.1.3)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.17.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->yellowbrick) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10.0->yellowbrick) (1.12.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (2.4.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (2.6.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=1.5.1->yellowbrick) (45.2.0)\n","Collecting category_encoders\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n","\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.1)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n","Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n","Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.25.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.17.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.6.1)\n","Installing collected packages: category-encoders\n","Successfully installed category-encoders-2.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"DkBiAq2-hFSY","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1583905395479,"user_tz":-330,"elapsed":17885,"user":{"displayName":"Chat Goo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjb90mL2nb5uUPlC7acaRzczH5y0WWbcyUz-dkuSw=s64","userId":"10971415490752751732"}},"outputId":"3731945b-8703-4064-d24d-e72caf0ed463"},"source":["#import various libraries and toolkits\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from numpy import loadtxt\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import category_encoders as ce\n","import warnings\n","%matplotlib inline\n","\n","from sklearn.utils.multiclass import unique_labels\n","from xgboost.sklearn import XGBClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from yellowbrick.classifier import ClassBalance, ROCAUC, ClassificationReport, ClassPredictionError\n","warnings.simplefilter('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"LxrjqtOuhFSn"},"source":["# function to plot confusion matrix (almost all of this was prebuilt. I made edits to fit my use-case)\n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    classes = classes[unique_labels(y_true, y_pred)]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","    \n","    plt.figure(figsize=(20,10))\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='Predicted label',\n","           xlabel='True label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    \n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    \n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"2q1NVpC6hFS2"},"source":["# PASS_NOpass_10k_0923.csv\n","# 2000 claims balanced.csv\n","# BAC_118659_1108.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"qV3ljdxFhFTE","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1583905789661,"user_tz":-330,"elapsed":14485,"user":{"displayName":"Chat Goo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjb90mL2nb5uUPlC7acaRzczH5y0WWbcyUz-dkuSw=s64","userId":"10971415490752751732"}},"outputId":"8665769c-37f5-43f1-d40b-197d6387145e"},"source":["trainset = input(\"What is the name of your training dataset?: \")\n","validationset = input('What is the name of your validation dataset?: ')\n","predictionset = input('What is the name of the prediction dataset?: ')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["What is the name of your training dataset?: BAC_118659_1108.csv\n","What is the name of your validation dataset?: BAC_118659_1108.csv\n","What is the name of the prediction dataset?: BAC_118659_1108.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"5_ZkmcDfhFTo","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1583905794179,"user_tz":-330,"elapsed":1446,"user":{"displayName":"Chat Goo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjb90mL2nb5uUPlC7acaRzczH5y0WWbcyUz-dkuSw=s64","userId":"10971415490752751732"}},"outputId":"f47a886b-849e-4823-bd95-ceeff5cbb033"},"source":["# Create function to read in testing dataset and predicting dataset\n","\n","def readinformattarget(trainset, predictionset):\n","    df = pd.read_csv(trainset, encoding = \"ISO-8859-1\")\n","    predictionset = pd.read_csv(predictionset, encoding = 'ISO-8859-1')\n","\n","    # Create list of df column names\n","    cols = list(df)\n","\n","    # Move 'Deviation Code F' to the front of the list of columns\n","    cols.insert(0, cols.pop(cols.index('Deviation Code F')))\n","    \n","    # Convert values of 'Deviation Code F' to 'non_compliant' or 'compliant' for vizualization purposes\n","    df.loc[(df['Deviation Code F'] != 'P'),'Deviation Code F']='Non-Compliant'\n","    df.loc[(df['Deviation Code F'] == 'P'),'Deviation Code F']='Compliant'\n","\n","    # Convert target variable to a category data type\n","    df['Deviation Code F'].astype('category')\n","    \n","    return df, predictionset\n","\n","# Assign training set and 'BAC to predict' to df and predictionset with intial formatting\n","df, predictionset = readinformattarget(trainset, predictionset)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9419eea404aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Assign training set and 'BAC to predict' to df and predictionset with intial formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadinformattarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-9419eea404aa>\u001b[0m in \u001b[0;36mreadinformattarget\u001b[0;34m(trainset, predictionset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadinformattarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictionset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'BAC_118659_1108.csv' does not exist: b'BAC_118659_1108.csv'"]}]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"-mZFoADRhFUq"},"source":["# Create function to make a bar chart to examine distribution of deviation codes within the training set (WILL NOT BE IN PRODUCTION SCRIPT)\n","def targetdistributionviz(df):\n","    \n","    # formatting\n","    ax2 = sns.countplot(df['Deviation Code F'], label='Count').set_title('distribution of compliant and non-compliant claims')\n","    plt.xlabel('Target Column: Deviation Code F')\n","    ax2.figure.savefig('Pass_fail_count.png')\n","    \n","# Execute    \n","targetdistributionviz(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"nlKljkv7hFVF"},"source":["# Create function to split the training set into X and y variables.\n","\n","def trainingsetsplit(trainfile):   \n","    # Re-initialize the training set as df for formatting purposes\n","    df = pd.read_csv(trainfile, encoding = \"ISO-8859-1\")\n","\n","    # Convert values of 'Deviation Code F' to 'non_compliant' or 'compliant' (0 or 1, respectively)\n","    df.loc[(df['Deviation Code F'] != 'P'),'Deviation Code F']='0'\n","    df.loc[(df['Deviation Code F'] == 'P'),'Deviation Code F']='1'\n","\n","    # Drop columns with duplicate names from our training dataset\n","    df = df.loc[:,~df.columns.duplicated()]\n","\n","    # Drop columns where all values are duplicates of another column\n","    df = df.T.drop_duplicates().T\n","\n","    # Drop columns where all values are the same\n","    nunique = df.apply(pd.Series.nunique)\n","    cols_to_drop = nunique[nunique == 1].index\n","    df.drop(cols_to_drop, axis=1)\n","\n","    # Split training set into feature set and response set\n","    X = df.drop('Deviation Code F',axis=1)\n","    y = df['Deviation Code F']\n","    y = y.to_frame()\n","    y = y.astype('int')\n","    return X, y\n","\n","X, y = trainingsetsplit(trainset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"W-yIJMsThFVY"},"source":["# Create running list of any columns that offer no statistical significance and drop them (will be updated as needed) (applying these drops to predictionset, because the final list of usable features must come from predictionset,\n","# as these are provided by our Audit Team partner, Cathy Snyder)\n","\n","predictionset = predictionset.drop(['CLAIM_NO', 'CLAIM_VERSION','Submitting_BAC'], axis = 1)\n","len(predictionset.dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ywss-qQPhFVp"},"source":["# Create a function to ensure that we are only using columns in our prediction set to train with\n","\n","def colmatching(original, predictionset, X):\n","    \n","    # create list of columns that exist in both data sets\n","    col_list = original.columns.intersection(predictionset.columns)\n","    \n","    # re-assign X and predictionset to account for list of similar columns\n","    X = X[col_list]\n","    predictionset = predictionset[col_list]\n","    return X, predictionset\n","\n","# Execute function\n","X, predictionset = colmatching(df, predictionset, X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"jG4IgfudhFWZ"},"source":["# Recheck that both datasets have an identical number of columns\n","print(X.shape, predictionset.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"qLrt8f9UhFWr"},"source":["# Examine column types for X to facilitate column conversion to their proper datatypes\n","\n","X.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"b_D_OJGfhFW2"},"source":["# Manual feature engineering. Converting columns to proper datatype for further processing\n","\n","cat_columns = ['Trouble_Code', 'Labor_Operation_cd', 'Model_Year', 'LABOR_OP_DEP_TYPE_KEY', 'REPAIR_GROUP_KEY', 'CUSTOMER_COMPLAINT_CD_KEY', 'FI_WARRANTY_TYPE_KEY']\n","int_columns = [ 'Odometer_Reading', 'TOTAL_LABOR_ITEM_COUNT', 'TOTAL_MATERIAL_QUANTITY', 'TOTAL_MESSAGE_COUNT_E']\n","float_columns = ['Net_Item_Amt_Exc_Tax', 'Total_Tax', 'Other_Hours', 'Total_Hours', 'TOTAL_DEDUCTIBLE_AMT', 'TOTAL_NET_ITEM_TAX_AMT', 'TOTAL_TAX_AMOUNT', 'TOTAL_LABOR_HOURS', 'OTHER_LABOR_HOURS', 'TOTAL_SUPP_LABOR_HOURS']\n","X[cat_columns] = X[cat_columns].astype('category')\n","X[int_columns] = X[int_columns].astype('int')\n","X[float_columns] = X[float_columns].astype('float')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"GRiSSajPhFXD"},"source":["# Create function to perform hashing trick\n","\n","def hashingtrick(X):\n","    ce_hash = ce.HashingEncoder()\n","    hashed_X = ce_hash.fit_transform(X)\n","    return hashed_X\n","\n","# execute hashing trick and assign to hashed_X, then check datatypes for all columns for confirmation\n","hashed_X = hashingtrick(X)\n","hashed_X.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"kFqZRiGBhFXY"},"source":["# Process hashed_X and y through .Dmatrix function so xgboost can ingest the data\n","data_dmatrix = xgb.DMatrix(data = hashed_X, label = y)\n","\n","# Conduct another split (train/test split) in order to perform cross-validation techniques\n","X_train, X_test, y_train, y_test = train_test_split(hashed_X,y, test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"wdFuvd-nhFXo"},"source":["X_train.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"l5BqDXVBhFX1"},"source":["# Instantiate process for Parameter tuning using cross validation\n","xgb_clf = xgb.XGBClassifier(tree_method = \"exact\", predictor = \"cpu_predictor\", verbosity = 1,\n","                            objective = \"binary:logistic\")\n","\n","# Create parameter grid\n","parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n","               \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n","               \"max_depth\": [2, 4, 7, 10],\n","               \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n","               \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7], # I've recently read that a minimum of .8 is a best practice, but this is a new development. I've been afraid to change anything but was curious if you\n","                                                       # had any knowledge of this metric?\n","               \"reg_alpha\": [0, 0.5, 1],\n","               \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n","               \"min_child_weight\": [1, 3, 5, 7],\n","               \"n_estimators\": [100, 250, 500, 1000]}\n","\n","\n"],"execution_count":null,"outputs":[]}]}